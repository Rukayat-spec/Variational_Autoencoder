<p><strong><u>Exploring Variational Autoencoders (VAEs): A Deep Dive into Latent Representations and Data Generation</u></strong></p>
<p><strong>NAME: RUKAYAT ARAMIDE IBRAHIM</strong></p>
<p><strong>STUDENT NO: 2200481</strong></p>
<p><strong>&nbsp;</strong></p>
<p><strong><u>Introduction</u></strong></p>
<p><strong><u>&nbsp;</u></strong></p>
<p>One of the most potent and adaptable tools in generative modelling are variational autoencoders (VAEs), which combine neural network designs with probabilistic techniques. VAEs are examined in this tutorial, which covers their applications, mathematical underpinnings, architecture, and implementation. You will get an understanding of VAEs and practical experience with their implementation at the conclusion, enabling you to apply them in your own work.</p>
<ol>
<li><strong> What is an Autoencoder?</strong></li>
</ol>
<p>An artificial neural network type called an autoencoder is made to develop effective representations of input data, often known as latent representations. By teaching the network to reconstruct the input data from its compressed, encoded form, it accomplishes this. Since autoencoders learn directly from the data by minimising reconstruction error, they are regarded as unsupervised learning models because they do not require labelled input.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>Fig&nbsp; 1: <em>A pictorial representation of an autoencoder network model (Wang et al., 2023).</em></p>
<p>&nbsp;</p>
<p><strong>Structure of an Autoencoder</strong></p>
<p>An autoencoder consists of two main parts:</p>
<p><strong>Encoder:</strong></p>
<ol>
<ul>
<li>The encoder compresses the input data into a compact, lower-dimensional latent representation.</li>
<li>This process reduces the data dimensionality and captures its most important features, essentially learning a compressed summary of the input.</li>
<li>Mathematically, the encoder maps the input x to the latent representation z, <em>z=f(x)</em>, where <em>f</em> is a function defined by the encoder&rsquo;s neural network layers.</li>
</ul>
</ol>
<p><strong>Decoder:</strong></p>
<ol>
<ul>
<li>The decoder takes the latent representation <em>z</em> and reconstructs it back to the original data space.</li>
<li>It attempts to regenerate an output <em>x^</em> that closely resembles the input <em>x</em>.</li>
<li>Mathematically, the decoder maps <em>z</em> to<em> x^, x^=g(z),</em> where g is the function defined by the decoder&rsquo;s neural network layers.</li>
</ul>
</ol>
<p>Together, the encoder and decoder form a symmetric architecture that learns how to compress and reconstruct the data.</p>
<p><strong>Applications of Autoencoders</strong></p>
<ul>
<li><strong>Data Compression: </strong>By compressing high-dimensional data (such pictures or movies) into a lower-dimensional latent space, autoencoders can improve the efficiency of data transmission and storage. An image can be compressed into a smaller vector, for instance, to conserve storage space without significantly sacrificing information.</li>
<li><strong>Noise Reduction: </strong>By training on both clean and noisy versions of the input, autoencoders can learn to remove noise from data. One common variation created especially for this use is denoising autoencoders.</li>
<li><strong>Anomaly Detection: </strong>Autoencoders can recognise anomalies as situations where reconstruction error is high by learning to reconstruct typical patterns in data. This application is frequently used in medical diagnostics, fraud detection, and industrial defect detection.</li>
<li><strong>Feature Extraction: </strong>The key characteristics of the data are captured by the encoder's learnt latent space representation, which can then be fed into other machine learning algorithms.</li>
</ul>
<p><strong>Limitations of Standard Autoencoders</strong></p>
<ul>
<li><strong>Deterministic Nature: </strong>For given inputs, standard autoencoders generate fixed, predictable latent representations. For identical inputs, the hidden representation remains constant after training. This restricts their capacity to produce a range of outputs or investigate data variations.</li>
<li><strong>Generative Modeling: </strong>Conventional autoencoders are not generative by nature; they are unable to produce novel, significant examples that closely resemble the training set.</li>
</ul>
<p>Variational Autoencoders (VAEs), which incorporate a probabilistic framework into the architecture, were made possible by these constraints. Better generative modelling and seamless data space exploration are made possible by VAEs, which learn a distribution in the latent space as opposed to fixed points.</p>
<p>&nbsp;</p>
<ol start="2">
<li><strong> Introduction to Variational Autoencoders (VAEs)</strong></li>
</ol>
<p>A probabilistic framework for latent space representation is incorporated into Variational Autoencoders (VAEs), which are an extension of ordinary autoencoders. VAEs translate inputs into probability distributions in the latent space as opposed to typical autoencoders, which encode inputs into fixed latent points. Multivariate Gaussian distributions with a mean vector <em>(&mu;)</em> and a standard deviation vector <em>(&sigma;)</em> are commonly used to parameterise these distributions. VAEs are effective tools for generative modelling because of their probabilistic methodology, which allows them to sample from these distributions.</p>
<p><strong>Why Use VAEs?</strong></p>
<ul>
<li><strong>Generative Modeling: </strong>New data points that mirror the training data can be produced using VAEs. For instance, a VAE may produce realistic new digit images given a dataset of handwritten digits.</li>
<li><strong>Probabilistic Interpretation: </strong>VAEs provide insights into uncertainty and underlying patterns by capturing the inherent variability in the data through the encoding of data into distributions.</li>
<li><strong>Smooth Latent Space: </strong>Smooth transitions between generated samples are made possible by the continuous and structured latent space in VAEs. For uses like interpolating between two data points or visual morphing, this feature is essential.</li>
</ul>
<p>Because VAEs' probabilistic nature makes both representation learning and data generation easier, they are frequently utilised in applications including picture synthesis, text generation, and anomaly detection.</p>
<ol start="3">
<li><strong> Key Components of VAEs</strong></li>
</ol>
<p>A VAE comprises three primary components: an <strong>encoder</strong>, a <strong>decoder</strong>, and a <strong>latent space</strong>.</p>
<p>Fig&nbsp; 2: <em>Image of a Variational Autoencoder architecture (</em><em>Kurtis Pykes, 2024).</em></p>
<p><strong>Encoder</strong></p>
<p>A probability distribution (such as Gaussian) defines the latent representation to which the encoder converts input data. Two parameters are output by it:</p>
<ul>
<li><strong>Mean (&mu;):</strong> The center of the distribution.</li>
<li><strong>Variance (&sigma;&sup2;):</strong> The spread of the distribution.</li>
</ul>
<p><strong>Latent Space</strong></p>
<p>A condensed, probabilistic representation of the input data is called the latent space. Each input is represented by a distribution rather than a fixed number, allowing for sampling and seamless data point transitions.</p>
<p><strong>Decoder</strong></p>
<p>Data from the latent space is reconstructed by the decoder. It takes a latent variable that has been sampled and produces output that is comparable to the initial input.</p>
<ol start="4">
<li><strong> Mathematical Framework</strong></li>
</ol>
<p><strong>Latent Variables and Probabilistic Modeling</strong></p>
<p>The VAE assumes that data <em>(x)</em> is generated from latent variables <em>(z)</em> via a likelihood distribution <em>p(x</em><em>∣</em><em>z).</em> The joint probability distribution is:</p>
<p><strong><em>p(x, z) = p(x|z) p(z)</em></strong></p>
<p>The goal is to learn the posterior distribution <em>p(z</em><em>∣</em><em>x),</em> which is computationally intractable. Instead, VAEs approximate it with <em>q(z</em><em>∣</em><em>x),</em> a learned distribution.</p>
<p><strong>Evidence Lower Bound (ELBO)</strong></p>
<p>To train a VAE, the objective is to maximize the <strong>Evidence Lower Bound (ELBO):</strong></p>
<p><strong><em>ELBO = Eq(z</em></strong><strong><em>∣ </em></strong><strong><em>x)[log(p(x</em></strong><strong><em>∣ </em></strong><strong><em>z))] &minus; KL(q(z</em></strong><strong><em>∣ </em></strong><strong><em>x)</em></strong><strong><em>∣∣</em></strong><strong><em> p(z))</em></strong></p>
<ul>
<li>The first term ensures accurate reconstruction.</li>
<li>The second term (Kullback-Leibler divergence) regularizes the latent space by aligning <em>q(z</em><em>∣</em><em>x)</em> with a prior distribution (usually a standard normal distribution).</li>
</ul>
<p><strong>KL Divergence</strong></p>
<p>The KL divergence quantifies the difference between two distributions:</p>
<p><strong><em>KL(q(z</em></strong><strong><em>∣ </em></strong><strong><em>x)</em></strong><strong><em>∣∣</em></strong><strong><em> p(z)) = Eq(z</em></strong><strong><em>∣ </em></strong><strong><em>x)[log(q(z</em></strong><strong><em>∣ </em></strong><strong><em>x)) &minus; log(p(z))]</em></strong></p>
<p>This term enforces smoothness and continuity in the latent space.</p>
<ol start="5">
<li><strong> Applications of VAEs</strong></li>
</ol>
<p>Combining generative modelling and representation learning, Variational Autoencoders (VAEs) are extremely flexible and have revolutionised a number of fields. They are indispensable in a variety of applications due to their capacity to create new data and encode existing data as probability distributions:</p>
<ul>
<li><strong>Image Generation: </strong>VAEs are very good at creating new, realistic images based on training datasets. By spotting patterns in image sets, VAEs can generate a range of outputs, such as: Designing games, creating artistic content, and transferring styles.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li><strong>Anomaly Detection: </strong>Because VAEs can learn the common patterns in datasets, they are ideal for identifying anomaly occurrences that significantly deviate from regular data. Here are some examples of applications: Cybersecurity, Quality Control, and Fraud Detection.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li><strong>Drug Discovery: </strong>VAEs support molecular design by learning chemical characteristics from familiar molecules and constructing novel molecular structures. They support accelerated research and virtual screening.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li><strong>Text Generation: </strong>In natural language processing (NLP), VAEs are effective at generating text that is both coherent and contextually relevant by learning from structured language datasets. Among the uses are language translation, content creation, and chatbots.</li>
</ul>
<p>These uses highlight VAEs' adaptability and efficiency in a variety of fields, spurring creativity and resolving challenging real-world issues.</p>
<h3><strong>6. Implementation of VAEs with Python</strong></h3>
<p>Let&rsquo;s build a VAE using Python and PyTouch with MNIST dataset</p>
<p><strong>Step 1: Import Necessary Libraries</strong></p>
<p>In order to facilitate data loading and batch processing for effective model training, this section imports PyTorch for deep learning, Torchvision for computer vision tasks and datasets, and DataLoader.</p>
<p><strong>Step 2: Prepare the Dataset</strong></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>The MNIST dataset is ready for testing and training in this part. Images are transformed into tensors and normalised using the standard deviation (0.3081) and mean (0.1307) of the dataset. The chosen transformations are applied after downloading and loading the training and testing datasets. After that, data loaders with a batch size of 64 are developed to enable effective batching. While the test loader preserves order, the training loader shuffles data for improved generalisation.</p>
<p><strong>Step</strong><strong> 3: Define the VAE Model</strong></p>
<p>&nbsp;</p>
<p>A Variational Autoencoder (VAE) with an encoder-decoder architecture is defined by this class. The encoder generates mean (mu) and log-variance (logvar) parameters by mapping input data to latent variables. In order to ensure backpropagation compatibility, the reparameterize technique uses these parameters to sample latent variables. The decoder uses the latent variables to reconstruct data. By collecting input data, encoding it, sampling latent variables, and producing reconstructed output, the forward approach combines these elements. Effective dimensionality reduction and generative modelling tasks are made possible by this architecture.</p>
<p><strong>Step</strong><strong> 4: Define the Loss Function and Training Loop</strong></p>
<p>&nbsp;</p>
<p>The Variational Autoencoder (VAE) training procedure is described in this section. The Kullback-Leibler divergence (KLD), which regularises the latent space, and binary cross-entropy (BCE), which measures reconstruction error, are combined in the loss function. The Adam optimiser with a learning rate of 1e-3 is used to optimise the VAE model after it has been initialised with particular input, latent, and hidden dimensions. To reduce reconstruction error and enhance latent space representation, the model iteratively adjusts parameters across ten epochs while processing data and calculating loss during training.</p>
<p><strong>Step 5: Generating New Images</strong></p>
<p>&nbsp;</p>
<p>In this step, the trained Variational Autoencoder (VAE) is used to create fresh samples. For efficiency, gradient computation is turned off in the torch.no_grad() context. The latent space of the VAE is represented by random latent vectors (z) of size 64x20 that are sampled from a conventional normal distribution. To create artificial images, these vectors are fed through the decoder. This stage shows that the VAE can generate new data by sampling from its learnt latent space, even though the visualisation function is not fully implemented here.</p>
<p><strong>Step</strong><strong> 6: Visualizing VAE Results on MNIST</strong></p>
<p>&nbsp;</p>
<p>Images produced by the trained Variational Autoencoder (VAE) are visualised by this code. Image data is obtained by sampling and decoding random latent vectors. Matplotlib is used to resize the photos to 28x28 pixels and display them in an 8x8 grid. This illustrates how the VAE may synthesise realistic facts from its latent space that it has learnt.</p>
<p>The picture shows how the VAE may produce a variety of handwritten numbers. The total outcome is remarkable, demonstrating the potential of VAEs for generative tasks, even though the generated images' quality varies.</p>
<p>The trained Variational Autoencoder's (VAE) latent space is visualised by this function. It stores the labels for the images in the test dataset by encoding them into latent vectors. Plotting of the latent vectors in two dimensions is done, with colours denoting various digit labels. This visualisation shows how the VAE groups related data points together, demonstrating its capacity to identify the dataset's underlying structure.</p>
<p>The way the VAE has learnt to encode MNIST digits into a meaningful latent space is shown visually in this visualisation. The latent space's smoothness and clustering show that the VAE has successfully caught the digits' underlying structure and is capable of producing lifelike images.</p>
<h3><strong>7. Challenges and Future Directions</strong></h3>
<h4><strong>Challenges</strong></h4>
<p>The performance and adoption of Variational Autoencoders (VAEs) are restricted by a number of technological obstacles. Balancing the KL divergence (ensuring smooth latent space distributions) and the reconstruction loss (accuracy in recreating input data) is a significant difficulty. Finding this equilibrium is crucial, but it frequently calls for fine adjustment. Another difficulty is producing varied and high-quality outputs, since VAEs can occasionally yield outcomes that are less clear or hazy than those of other generative models, such as GANs.</p>
<h4><strong>Future Directions</strong></h4>
<p>Future developments seek to get around these restrictions by investigating hybrid strategies like VAE-GANs, which combine the adversarial training of GANs with the probabilistic structure of VAEs to improve output diversity and realism. Additionally, the application breadth of VAEs can be increased by creating domain-specific architectures. For example, specialised models for audio data can enhance voice creation and music synthesis, while architectures designed for 3D data can enhance applications in virtual reality and robotics.</p>
<p><strong>References</strong></p>
<p>Bergmann, D., &amp; Stryker, C. (2024). Autoencoder. IBM. Available at: <a href="https://www.ibm.com/topics/autoencoder">https://www.ibm.com/topics/autoencoder</a> (Accessed: 31, November 2024).</p>
<p>Choudhary, A. S. (2024). An Overview of Variational Autoencoders (VAEs). Analytics Vidhya. Available at: <a href="https://www.analyticsvidhya.com/blog/2023/07/an-overview-of-variational-autoencoders/">https://www.analyticsvidhya.com/blog/2023/07/an-overview-of-variational-autoencoders/</a> (Accessed: 28, November 2024). &nbsp;</p>
<p>Jatasra, S. (2023). Unravelling the Mysteries of Variational Autoencoders: A Deep Dive into Intelligent Data Generation. Available at:<a href="https://www.linkedin.com/pulse/unravelling-mysteries-variational-autoencoders-deep-dive-jatasra">https://www.linkedin.com/pulse/unravelling-mysteries-variational-autoencoders-deep-dive-jatasra</a> (Accessed: 25, November 2024)</p>
<p>Kurtis P. (2024). Variational Autoencoders: How They Work and Why They Matter. Available at: <a href="https://www.datacamp.com/tutorial/variational-autoencoders">https://www.datacamp.com/tutorial/variational-autoencoders</a> (Accessed: 4, December 2024).</p>
<p>Olaoye, G., Luz, A. and Charles, E., (2024). &ldquo;Variational autoencoders (VaEs) for synthetic data generation&rdquo; <a href="https://www.researchgate.net/publication/385592671_Variational_autoencoders_vaes_for_synthetic_data_generation">https://www.researchgate.net/publication/385592671_Variational_autoencoders_vaes_for_synthetic_data_generation</a></p>
<p>Wang, Q., Qin, K., Lu, B.&nbsp;<em>et al.</em>, (2023). &ldquo;Time-feature attention-based convolutional auto-encoder for flight feature extraction&rdquo;.&nbsp;<em>Sci Rep&nbsp;<strong>13</strong>, 14175</em>. <a href="https://doi.org/10.1038/s41598-023-41295-y">https://doi.org/10.1038/s41598-023-41295-y</a></p>
<p>&nbsp;</p>
