\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Exploring Variational Autoencoders (VAEs): A Deep Dive into Latent Representations and Data Generation}
\author{RUKAYAT ARAMIDE IBRAHIM}
\date{December 2024}

\begin{document}

\maketitle

\section{Introduction}
One of the most potent and adaptable tools in generative modelling are variational autoencoders (VAEs), which combine neural network designs with probabilistic techniques. VAEs are examined in this tutorial, which covers their applications, mathematical underpinnings, architecture, and implementation. You will get an understanding of VAEs and practical experience with their implementation at the conclusion, enabling you to apply them in your own work.


\section{ What is an Autoencoder?}

 \begin{figure}
     \centering
     \includegraphics[width=0.8\linewidth]{image.png}
     \caption{A pictorial representation of an autoencoder network model (Wang et al., 2023).}
 \end{figure}
 
An artificial neural network type called an autoencoder is made to develop effective representations of input data, often known as latent representations. By teaching the network to reconstruct the input data from its compressed, encoded form, it accomplishes this. Since autoencoders learn directly from the data by minimising reconstruction error, they are regarded as unsupervised learning models because they do not require labelled input.

\subsection{Structure of an Autoencoder}
An autoencoder consists of two main parts:
\subsubsection{Encoder:}
\begin{itemize}
\item	The encoder compresses the input data into a compact, lower-dimensional latent representation.
\item This process reduces the dimensionality of the data and captures its most important features, essentially learning a compressed summary of the input.
\item	Mathematically, the encoder maps the input x to the latent representation z, z=f(x), where f is a function defined by the encoder’s neural network layers.
\end{itemize}
\subsubsection{Decoder:}
\begin{itemize}
\item The decoder takes the latent representation z and reconstructs it back to the original data space.
\item   It attempts to regenerate an output \(x^\) that closely resembles the input x.
\item  Mathematically, the decoder maps z to \(x^, x^=g(z\), where g is the function defined by the decoder’s neural network layers.
\end{itemize}
Together, the encoder and decoder form a symmetric architecture that learns how to compress and reconstruct the data.
\subsection{Applications of Autoencoders}
\begin{itemize}
\item Data Compression: By compressing high-dimensional data (such pictures or movies) into a lower-dimensional latent space, autoencoders can improve the efficiency of data transmission and storage. An image can be compressed into a smaller vector, for instance, to conserve storage space without significantly sacrificing information.
\item	Noise Reduction: By training on both clean and noisy versions of the input, autoencoders can learn to remove noise from data. One common variation created especially for this use is denoising autoencoders.
\item Anomaly Detection: Autoencoders can recognise anomalies as situations where reconstruction error is high by learning to reconstruct typical patterns in data. This application is frequently used in medical diagnostics, fraud detection, and industrial defect detection.
\item	Feature Extraction: The key characteristics of the data are captured by the encoder's learnt latent space representation, which can then be fed into other machine learning algorithms.
\end{itemize}

\subsection{Limitations of Standard Autoencoders}
\begin{itemize}

\item Deterministic Nature: For given inputs, standard autoencoders generate fixed, predictable latent representations. For identical inputs, the hidden representation remains constant after training. This restricts their capacity to produce a range of outputs or investigate data variations.
\item Generative Modeling: Conventional autoencoders are not generative by nature; they are unable to produce novel, significant examples that closely resemble the training set.


Variational Autoencoders (VAEs), which incorporate a probabilistic framework into the architecture, were made possible by these constraints. Better generative modelling and seamless data space exploration are made possible by VAEs, which learn a distribution in the latent space as opposed to fixed points.
\end{itemize}
\end{document}
